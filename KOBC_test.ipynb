{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 0. 모델 캐시 디렉토리 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 모델과 토크나이저를 먼저 불러오면서 캐시 디렉터리 설정\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-9b-it\", \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    cache_dir=\"/workspace/youngwoo/KOBCLLM/cache\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"google/gemma-2-9b-it\", \n",
    "    cache_dir=\"/workspace/youngwoo/KOBCLLM/cache\"\n",
    ")\n",
    "\n",
    "# 파이프라인 설정\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=\"cuda\",  # replace with \"mps\" to run on a Mac device\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you? Please, answer in pirate-speak.\"},\n",
    "]\n",
    "\n",
    "outputs = pipe(messages, max_new_tokens=256)\n",
    "assistant_response = outputs[0][\"generated_text\"][-1][\"content\"].strip()\n",
    "print(assistant_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 1. 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = '/workspace/youngwoo/KOBCLLM/resource/신조선가/BULKER.xls' \n",
    "bulker_df = pd.read_excel(file_path, skiprows=1, engine='xlrd')\n",
    "bulker_df = bulker_df.drop(columns=['번호'])\n",
    "bulker_df = bulker_df.set_index('DATE')\n",
    "\n",
    "print(bulker_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You're a helpful assistant\"\"\"\n",
    "\n",
    "analysis_prompt = \"\"\"당신은 데이터 분석 전문가입니다.\n",
    "\n",
    "아래의 데이터 프레임은 BULKER선의 {COLUMNS}들의 신조선가 데이터 입니다.\n",
    "{DATAFRAME}\n",
    "\n",
    "사용자 요청에 대해 해당 데이터를 활용해 답변하세요.\n",
    "\n",
    "# 사용자 요청\n",
    "{MESSAGE}\n",
    "\n",
    "답변은 분석한 결과 이외에 하지 않습니다.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prompt_analysis_test(STATE):\n",
    "    prompt = [\n",
    "        SystemMessage(content=SYSTEM_PROMPT),\n",
    "        HumanMessage(\n",
    "            content=analysis_prompt.format_map(STATE)\n",
    "        ),\n",
    "    ]\n",
    "    return prompt\n",
    "\n",
    "# 모델과 토크나이저를 사용한 파이프라인 생성\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-9b-it\", \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    cache_dir=\"/workspace/youngwoo/KOBCLLM/cache\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"google/gemma-2-9b-it\", \n",
    "    cache_dir=\"/workspace/youngwoo/KOBCLLM/cache\"\n",
    ")\n",
    "\n",
    "gemma_llm = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=\"cuda\"  # 필요한 경우, 'mps'로 변경\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 준비 및 체인 실행\n",
    "MESSAGE = \"2년간 벌크선의 신조선가 추이에 대해 설명해주세요.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for |: 'function' and 'TextGenerationPipeline'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 10\u001b[0m\n\u001b[1;32m      2\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m         SystemMessage(content\u001b[38;5;241m=\u001b[39mSYSTEM_PROMPT),\n\u001b[1;32m      4\u001b[0m         HumanMessage(\n\u001b[1;32m      5\u001b[0m             content\u001b[38;5;241m=\u001b[39manalysis_prompt\u001b[38;5;241m.\u001b[39mformat_map(STATE)\n\u001b[1;32m      6\u001b[0m         ),\n\u001b[1;32m      7\u001b[0m     ]\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m prompt\n\u001b[0;32m---> 10\u001b[0m Chain__analysis_test \u001b[38;5;241m=\u001b[39m \u001b[43mprompt_analysis_test\u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43mgemma_llm\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for |: 'function' and 'TextGenerationPipeline'"
     ]
    }
   ],
   "source": [
    "def prompt_analysis_test(STATE):\n",
    "    prompt = [\n",
    "        SystemMessage(content=SYSTEM_PROMPT),\n",
    "        HumanMessage(\n",
    "            content=analysis_prompt.format_map(STATE)\n",
    "        ),\n",
    "    ]\n",
    "    return prompt\n",
    "\n",
    "Chain__analysis_test = prompt_analysis_test | gemma_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Chain__analysis_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m ANSWER \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[43mChain__analysis_test\u001b[49m\u001b[38;5;241m.\u001b[39mstream({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMESSAGE\u001b[39m\u001b[38;5;124m\"\u001b[39m:MESSAGE,\n\u001b[1;32m      3\u001b[0m                                         \u001b[38;5;66;03m#   \"DATAFRAME\":df.to_markdown(index=False),\u001b[39;00m\n\u001b[1;32m      4\u001b[0m                                           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATAFRAME\u001b[39m\u001b[38;5;124m\"\u001b[39m:bulker_df\u001b[38;5;241m.\u001b[39mto_html(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m      5\u001b[0m                                           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCOLUMNS\u001b[39m\u001b[38;5;124m\"\u001b[39m:bulker_df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mto_list()}):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[38;5;12m\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m chunk\u001b[38;5;241m.\u001b[39mcontent \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[0m\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Chain__analysis_test' is not defined"
     ]
    }
   ],
   "source": [
    "ANSWER = \"\"\n",
    "for chunk in Chain__analysis_test.stream({\"MESSAGE\":MESSAGE,\n",
    "                                        #   \"DATAFRAME\":df.to_markdown(index=False),\n",
    "                                          \"DATAFRAME\":bulker_df.to_html(index=False),\n",
    "                                          \"COLUMNS\":bulker_df.columns.to_list()}):\n",
    "    print(\"\\033[38;5;12m\" + chunk.content + \"\\033[0m\", end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
